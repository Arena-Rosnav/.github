<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../../img/favicon.ico" />
    <title>Training - Arena Rosnav</title>
    <link rel="stylesheet" href="../../css/theme.css" />
    <link rel="stylesheet" href="../../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/styles/github.min.css" />
        <link href="../../css/extra.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Training";
        var mkdocs_page_input_path = "user_guides/training.md";
        var mkdocs_page_url = null;
      </script>
    
    <script src="../../js/jquery-3.6.0.min.js" defer></script>
    <!--[if lt IE 9]>
      <script src="../../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/highlight.min.js"></script>
      <script>hljs.initHighlightingOnLoad();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="../.." class="icon icon-home"> Arena Rosnav
        </a>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../..">Introduction</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../../publications/">Publications</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../../contribute/">Contribute</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">User Guides</span></p>
              <ul class="current">
                  <li class="toctree-l1"><a class="reference internal" href="../installation/">Installation</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../usage/">Usage</a>
                  </li>
                  <li class="toctree-l1 current"><a class="reference internal current" href="./">Training</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#start-the-training">Start the Training</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#training-script">Training Script</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#usage">Usage</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#examples">Examples</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#training-with-a-predefined-dnn">Training with a predefined DNN</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#load-a-dnn-for-training">Load a DNN for training</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#training-with-a-custom-mlp">Training with a custom MLP</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#multiprocessed-training">Multiprocessed Training</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#hyperparameters">Hyperparameters</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#reward-functions">Reward Functions</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#rule-0">Rule 0</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#rule-1">Rule 1</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#rule-2">Rule 2</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#rule-3">Rule 3</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#rule-4">Rule 4</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#training-curriculum">Training Curriculum</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#run-the-trained-agent">Run the trained Agent</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#test-agents-in-main-simulation">Test Agents in Main Simulation</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#test-agents-in-training-simulation">Test Agents in Training Simulation</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#sequential-evaluation-of-multiple-agents">Sequential Evaluation of multiple Agents</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#pipeline-components">Pipeline Components</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#important-directories">Important Directories</a>
    </li>
    </ul>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Packages</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../packages/task_generator/">Task Generator</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../packages/arena_simulation_setup/">Arena Simulation Setup</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../packages/arena_evaluation/">Evaluation</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../packages/rosnavrl/">ROSNavRL</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Tutorials</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../tutorials/add_new_environment/">Add new environment</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../tutorials/train_agent_with_different_observation_spaces/">Train an agent with different observation spaces</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../..">Arena Rosnav</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../.." class="icon icon-home" alt="Docs"></a> &raquo;</li>
          <li>User Guides &raquo;</li><li>Training</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>

          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="training">Training</h1>
<p>As a fundament for our Deep Reinforcement Learning approaches, <a href="https://stable-baselines3.readthedocs.io/en/master/index.html">StableBaselines3</a> was used.</p>
<p><strong>Features included so far:</strong></p>
<ul>
<li>Simple handling of the training script through program parameters</li>
<li>Choose between different robot models</li>
<li>Choose a predefined Deep Neural Network</li>
<li>Create your own custom Multilayer Perceptron via program parameters</li>
<li>Networks will be trained, evaluated and saved</li>
<li>Load your trained agent to continue training</li>
<li>Optionally log training and evaluation data</li>
<li>Enable and modify a custom training curriculum</li>
<li>Multiprocessed rollout collection for training</li>
</ul>
<h2 id="start-the-training">Start the Training</h2>
<p>To start a training procedure you need two terminals.</p>
<p>In terminal 1 start the simulation environment:</p>
<pre><code>roslaunch arena_bringup start_training.launch
</code></pre>
<p>In terminal 2 run the training script:</p>
<pre><code>cd arena-rosnav # navigate to the arena-rosnav directory
python training/scripts/train_agent.py --agent AGENT_22
</code></pre>
<h2 id="training-script">Training Script</h2>
<h3 id="usage">Usage</h3>
<p><strong>Generic program call</strong>:</p>
<pre><code>train_agent.py [agent flag] [agent_name | unique_agent_name | custom mlp params] [optional flag] [optional flag] ...
</code></pre>
<table>
<thead>
<tr>
<th>Program call</th>
<th>Agent Flag (mutually exclusive)</th>
<th>Usage</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>train_agent.py</code></td>
<td><code>--agent</code></td>
<td><em>agent_name</em> (<a href="#training-with-a-predefined-dnn">see below</a>)</td>
<td>initializes a predefined network from scratch</td>
</tr>
<tr>
<td></td>
<td><code>--load</code></td>
<td><em>unique_agent_name</em> (<a href="#load-a-dnn-for-training">see below</a>)</td>
<td>loads agent to the given name</td>
</tr>
<tr>
<td></td>
<td><code>--custom-mlp</code></td>
<td><em>custom_mlp_params</em> (<a href="#training-with-a-custom-mlp">see below</a>)</td>
<td>initializes custom MLP according to given arguments</td>
</tr>
</tbody>
</table>
<p><em>Custom Multilayer Perceptron</em> parameters will only be considered when <code>--custom-mlp</code> was set!</p>
<table>
<thead>
<tr>
<th>Custom Mlp Flags</th>
<th>Syntax</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>--body</code></td>
<td><code>{num}-{num}-...</code></td>
<td>architecture of the shared latent network</td>
</tr>
<tr>
<td><code>--pi</code></td>
<td><code>{num}-{num}-...</code></td>
<td>architecture of the latent policy network</td>
</tr>
<tr>
<td><code>--vf</code></td>
<td><code>{num}-{num}-...</code></td>
<td>architecture of the latent value network</td>
</tr>
<tr>
<td><code>--act_fn</code></td>
<td><code>{relu, sigmoid or tanh}</code></td>
<td>activation function to be applied after each hidden layer</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th>Optional Flags</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>--config {string}</code>, defaults to <em>"default"</em></td>
<td>Looks for the given config file name in <a href="/arena-rosnav/arena_navigation/arena_local_planner/learning_based/arena_local_planner_drl/configs/hyperparameters">../arena_local_planner_drl/configs/hyperparameters</a> to load the configurations from</td>
</tr>
<tr>
<td><code>--n {integer}</code></td>
<td>timesteps in total to be generated for training</td>
</tr>
<tr>
<td><code>--tb</code></td>
<td>enables tensorboard logging</td>
</tr>
<tr>
<td><code>-log</code>, <code>--eval_log</code></td>
<td>enables logging of evaluation episodes</td>
</tr>
<tr>
<td><code>--no-gpu</code></td>
<td>disables training with GPU</td>
</tr>
<tr>
<td><code>--num_envs {integer}</code></td>
<td>number of environments to collect experiences from for training (for more information refer to <a href="#multiprocessed-training">Multiprocessed Training</a>)</td>
</tr>
</tbody>
</table>
<h3 id="examples">Examples</h3>
<h4 id="training-with-a-predefined-dnn">Training with a predefined DNN</h4>
<p>Currently you can choose between several different Deep Neural Networks each of which have been object of research projects, for example:</p>
<table>
<thead>
<tr>
<th>Agent name</th>
<th>Inspired by</th>
</tr>
</thead>
<tbody>
<tr>
<td>MLP_ARENA2D</td>
<td><a href="https://github.com/ignc-research/arena2D">arena2D</a></td>
</tr>
<tr>
<td>DRL_LOCAL_PLANNER</td>
<td><a href="https://github.com/RGring/drl_local_planner_ros_stable_baselines">drl_local_planner</a></td>
</tr>
<tr>
<td>CNN_NAVREP</td>
<td><a href="https://github.com/ethz-asl/navrep">NavRep</a></td>
</tr>
</tbody>
</table>
<p>e.g. training with the MLP architecture from arena2D:</p>
<pre><code>train_agent.py --agent MLP_ARENA2D
</code></pre>
<p>You can find the most recently implemented neural network architectures in: <a href="/arena-rosnav/arena_navigation/arena_local_planner/learning_based/arena_local_planner_drl/scripts/custom_policy.py">custom_policy.py</a></p>
<h4 id="load-a-dnn-for-training">Load a DNN for training</h4>
<p>In order to differentiate between agents with similar architectures but from different runs a unique agent name will be generated when using either <code>--agent</code> or <code>--custom-mlp</code> mode (when train from scratch).</p>
<p>The name consists of:</p>
<pre><code>[architecture]_[year]_[month]__[hour]_[minute]
</code></pre>
<p>To load a specific agent you simply use the flag <code>--load</code>, e.g.:</p>
<pre><code>train_agent.py --load MLP_ARENA2D_2021_01_19__03_20
</code></pre>
<p><strong>Note</strong>: currently only agents which were trained with PPO given by StableBaselines3 are compatible with the training script.</p>
<h4 id="training-with-a-custom-mlp">Training with a custom MLP</h4>
<p>Instantiating a MLP architecture with an arbitrary number of layers and neurons for training was made as simple as possible by providing the option of using the <code>--custom-mlp</code> flag. By typing in the flag additional flags for the architecture of latent layers get accessible (<a href="#program-arguments">see above</a>).</p>
<p>e.g. given following architecture:</p>
<pre><code>                       obs
                        |
                      &lt;256&gt;
                        |
                      ReLU
                        |
                      &lt;128&gt;
                        |
                      ReLU
                    /               \
                 &lt;256&gt;             &lt;16&gt;
                   |                 |
                 action            value
</code></pre>
<p>program must be invoked as follows:</p>
<pre><code>train_agent.py --custom-mlp --body 256-128 --pi 256 --vf 16 --act_fn relu
</code></pre>
<h3 id="multiprocessed-training">Multiprocessed Training</h3>
<p>We provide for either testing and training purposes seperate launch scripts:</p>
<ul>
<li><code>start_arena_flatland.launch</code> encapsulates the simulation environment featuring the different intermediate planners in a single process. Training is also possible within this simulation.</li>
<li><code>start_training.launch</code> depicts the slimer simulation version as we target a higher troughput here in order to be able to gather training data as fast as possible. The crucial feature of this launch file is that it is able to spawn an arbitrary number of environments to collect the rollouts with and thus allows for significant speedup through asynchronicity.</li>
</ul>
<p><strong>First terminal: Simulation</strong>
The first terminal is needed to run arena.</p>
<p>Run these commands:</p>
<pre><code class="language-shell">workon rosnav
roslaunch arena_bringup start_training.launch train_mode:=true use_viz:=false task_mode:=random map_file:=map_small num_envs:=4
</code></pre>
<p><strong>Second terminal: Training script</strong>
A second terminal is needed to run the training script.</p>
<ul>
<li>Run these four commands:</li>
</ul>
<pre><code class="language-shell">workon rosnav
roscd arena_local_planner_drl
</code></pre>
<ul>
<li>Now, run one of the two commands below to start a training session:</li>
</ul>
<pre><code class="language-shell">python scripts/training/train_agent.py --load pretrained_ppo_mpc --n_envs 4 --eval_log
python scripts/training/train_agent.py --load pretrained_ppo_baseline --n_envs 4 --eval_log
</code></pre>
<p><strong>Note</strong>: Please inform yourself how many cores are provided by your processor in order to fully leverage local computing capabilities.</p>
<p><strong>Third terminal: Visualization</strong>
A third terminal is needed in order to start rviz for visualization.</p>
<ul>
<li>Run this command:</li>
</ul>
<pre><code class="language-shell">roslaunch arena_bringup visualization_training.launch ns:=*ENV NAME*
</code></pre>
<p><strong>Note</strong>: The training environments start with prefix <em>sim</em> and end with the index. For example: <em>sim_1</em>, <em>sim_2</em> and so on. The evaluation environment which is used during the periodical benchmarking in training can be shown with <code>ns:=eval_sim</code>.</p>
<p><strong>Ending a training session</strong></p>
<p>When the training script is done, it will print the following information and then exit:</p>
<pre><code>Time passed: {time in seconds}s
Training script will be terminated
</code></pre>
<h2 id="hyperparameters">Hyperparameters</h2>
<p>The training script will consider the hyperparameter yaml file which was specified with the <code>--config</code> flag. The default configuration file is named <code>default.yaml</code> and can be found at:</p>
<pre><code class="language-bash">~/catkin_ws/src/arena-rosnav/arena_navigation/arena_local_planner/learning_based/arena_local_planner_drl/configs/hyperparameters/default.json
</code></pre>
<p>Following hyperparameters can be adapted:</p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>agent_name</td>
<td>Unique agent identifier (set by the training script)</td>
</tr>
<tr>
<td>robot</td>
<td>Robot name to load robot specific .yaml file containing its settings. <br />(set by the training script)</td>
</tr>
<tr>
<td>actions_in_observationspace</td>
<td>When set true, actions will be appended to the observation space and<br /> thus will be available as an additional input feature to learn on.<br /> Noticeable performance improvements when utilizing reward function<br /> that incorporates robot velocity.</td>
</tr>
<tr>
<td>normalize</td>
<td>If observations are normalized before fed to the network</td>
</tr>
<tr>
<td>train_max_steps_per_episode</td>
<td>Max timesteps per training episode</td>
</tr>
<tr>
<td>eval_max_steps_per_episode</td>
<td>Max timesteps per evaluation episode</td>
</tr>
<tr>
<td>goal_radius</td>
<td>Radius of the goal</td>
</tr>
<tr>
<td>task_mode</td>
<td>Mode tasks will be generated in (custom, random, staged). In custom<br /> mode one can place obstacles manually via Rviz. In random mode there's<br /> a fixed number of obstacles which are spawned randomly distributed on <br />the map after each episode. In staged mode the training curriculum will<br /> be used to spawn obstacles. (<a href="#training-curriculum">more info</a>)</td>
</tr>
<tr>
<td>batch_size</td>
<td>Batch size (n_envs * n_steps)</td>
</tr>
<tr>
<td>gamma</td>
<td>Discount factor</td>
</tr>
<tr>
<td>n_steps</td>
<td>The number of steps to run for each environment per update<br /> (set automatically by training script depending on batch_size and n_envs)</td>
</tr>
<tr>
<td>ent_coef</td>
<td>Entropy coefficient for the loss calculation</td>
</tr>
<tr>
<td>learning_rate</td>
<td>The learning rate, it can be a function of the current progress remaining <br />(from 1 to 0) (i.e. batch size is n_steps and n_env where n_env is number<br /> of environment copies running in parallel)</td>
</tr>
<tr>
<td>vf_coef</td>
<td>Value function coefficient for the loss calculation</td>
</tr>
<tr>
<td>max_grad_norm</td>
<td>The maximum value for the gradient clipping</td>
</tr>
<tr>
<td>gae_lambda</td>
<td>Factor for trade-off of bias vs variance for <br />Generalized Advantage Estimator</td>
</tr>
<tr>
<td>m_batch_size</td>
<td>Minibatch size</td>
</tr>
<tr>
<td>n_epochs</td>
<td>Number of epoch when optimizing the surrogate loss</td>
</tr>
<tr>
<td>clip_range</td>
<td>Clipping parameter, it can be a function of the current progress remaining<br /> (from 1 to 0).</td>
</tr>
<tr>
<td>reward_fnc</td>
<td>Number of the reward function (defined in *../rl_agent/utils/reward.py*)</td>
</tr>
<tr>
<td>discrete_action_space</td>
<td>If robot uses discrete action space</td>
</tr>
<tr>
<td>curr_stage</td>
<td>When "staged" training is activated: which stage to start the<br /> training with.</td>
</tr>
</tbody>
</table>
<p>(<a href="https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html">more information on PPO implementation of SB3</a>)</p>
<p><strong>Note</strong>: For now further parameters like the threshold type for the curriculum or the number of eval episodes need to be specified inline in the training script.</p>
<h2 id="reward-functions">Reward Functions</h2>
<p>The reward functions are defined in:</p>
<pre><code>../arena_local_planner_drl/rl_agent/utils/reward.py
</code></pre>
<p>At present, one can chose between five reward functions which can be set in the <a href="#hyperparameters">hyperparameters yaml file</a>:</p>
<!-- <table>
<tr>
   <th>rule_00</th> <th>rule_01</th> <th>rule_02</th> <th>rule_03</th> <th>rule_04</th>
</tr>
<tr>

  <td> -->

<h4 id="rule-0">Rule 0</h4>
<p><img src="https://latex.codecogs.com/gif.latex?r_{00}^{t}&space;=&space;r_{s}^{t}&space;&plus;&space;r_{c}^{t}&space;&plus;&space;r_{d}^{t}&space;&plus;&space;r_{p}^{t}" title="r_{00}^{t} = r_{s}^{t} + r_{c}^{t} + r_{d}^{t} + r_{p}^{t}" /></p>
<table>
<thead>
<tr>
<th>reward</th>
<th>description</th>
<th>value</th>
</tr>
</thead>
<tbody>
<tr>
<td><img src="https://latex.codecogs.com/gif.latex?r_{s}^{t}" title="r_{s}^{t}" /></td>
<td>success reward</td>
<td><img src="https://latex.codecogs.com/gif.latex?r_{s}^{t}&space;=&space;\begin{cases}&space;15&space;&&space;\text{&space;if&space;goal&space;reached}&space;\92&space;0&space;&&space;\text{&space;otherwise&space;}&space;\end{cases}" title="r_{s}^{t} = \begin{cases} 15 & \text{ if goal reached} \92 0 & \text{ otherwise } \end{cases}" /></td>
</tr>
<tr>
<td><img src="https://latex.codecogs.com/gif.latex?r_{c}^{t}" title="r_{c}^{t}" /></td>
<td>collision reward</td>
<td><img src="https://latex.codecogs.com/gif.latex?r_{c}^{t}&space;=&space;\begin{cases}&space;-10&space;&&space;\text{&space;if&space;robot&space;collides}&space;\92&space;0&space;&&space;\text{&space;otherwise&space;}&space;\end{cases}" title="r_{c}^{t} = \begin{cases} -10 & \text{ if robot collides} \92 0 & \text{ otherwise } \end{cases}" /></td>
</tr>
<tr>
<td><img src="https://latex.codecogs.com/gif.latex?r_{d}^{t}" title="r_{d}^{t}" /></td>
<td>danger reward</td>
<td>&lt;img src="https://latex.codecogs.com/gif.latex?r_{d}^{t}&space;=&space;\begin{cases}&space;-0.25&space;&amp;&space;\text{&space;if&space;}&space;\exists{o&space;\in&space;O}&space;:&space;d(p_{robot}^t,&space;p_{obs}^t)&space;&lt;&space;D_{s}\&space;0&space;&amp;&space;\text{&space;otherwise&space;}&space;\end{cases}" title="r_{d}^{t} = \begin{cases} -0.25 &amp; \text{ if } \exists{o \in O} : d(p_{robot}^t, p_{obs}^t) &lt; D_{s}\ 0 &amp; \text{ otherwise } \end{cases}" /&gt;</td>
</tr>
<tr>
<td><img src="https://latex.codecogs.com/gif.latex?r_{p}^{t}" title="r_{p}^{t}" /></td>
<td>progress reward</td>
<td><img src="https://latex.codecogs.com/gif.latex?\text{diff}_{robot,x}^t&space;=&space;d(p_{robot}^{t-1},&space;p_{x}^{t-1})&space;-&space;d(p_{robot}^t,&space;p_{x}^t)" title="\text{diff}_{robot,x}^t = d(p_{robot}^{t-1}, p_{x}^{t-1}) - d(p_{robot}^t, p_{x}^t)" /> <img src="https://latex.codecogs.com/gif.latex?r_{p}^{t}&space;=&space;\begin{cases}&space;0.3&space;*&space;\text{diff}_{robot,goal}^t&space;&&space;\text{&space;if&space;}&space;\text{diff}_{robot,goal}^t&space;>&space;0\&space;0.4&space;*&space;\text{diff}<em>{robot,goal}^t&space;&amp;&space;\text{&space;otherwise&space;}&space;\end{cases}" title="r</em>{p}^{t} = \begin{cases} 0.3 * \text{diff}<em>{robot,goal}^t &amp; \text{ if } \text{diff}</em>{robot,goal}^t &gt; 0\ 0.4 * \text{diff}_{robot,goal}^t &amp; \text{ otherwise } \end{cases}" /&gt;</td>
</tr>
</tbody>
</table>
<h4 id="rule-1">Rule 1</h4>
<p><img src="https://latex.codecogs.com/gif.latex?r_{01}^{t}&space;=&space;r_{s}^{t}&space;&plus;&space;r_{c}^{t}&space;&plus;&space;r_{d}^{t}&space;&plus;&space;r_{p}^{t}&space;&plus;&space;r_{dt}^{t}" title="r_{01}^{t} = r_{s}^{t} + r_{c}^{t} + r_{d}^{t} + r_{p}^{t} + r_{dt}^{t}" /></p>
<table>
<thead>
<tr>
<th>reward</th>
<th>description</th>
<th>value</th>
</tr>
</thead>
<tbody>
<tr>
<td><img src="https://latex.codecogs.com/gif.latex?r_{s}^{t}" title="r_{s}^{t}" /></td>
<td>success reward</td>
<td><img src="https://latex.codecogs.com/gif.latex?r_{s}^{t}&space;=&space;\begin{cases}&space;15&space;&&space;\text{&space;if&space;goal&space;reached}&space;\92&space;0&space;&&space;\text{&space;otherwise&space;}&space;\end{cases}" title="r_{s}^{t} = \begin{cases} 15 & \text{ if goal reached} \92 0 & \text{ otherwise } \end{cases}" /></td>
</tr>
<tr>
<td><img src="https://latex.codecogs.com/gif.latex?r_{c}^{t}" title="r_{c}^{t}" /></td>
<td>collision reward</td>
<td><img src="https://latex.codecogs.com/gif.latex?r_{c}^{t}&space;=&space;\begin{cases}&space;-10&space;&&space;\text{&space;if&space;robot&space;collides}&space;\92&space;0&space;&&space;\text{&space;otherwise&space;}&space;\end{cases}" title="r_{c}^{t} = \begin{cases} -10 & \text{ if robot collides} \92 0 & \text{ otherwise } \end{cases}" /></td>
</tr>
<tr>
<td><img src="https://latex.codecogs.com/gif.latex?r_{d}^{t}" title="r_{d}^{t}" /></td>
<td>danger reward</td>
<td>&lt;img src="https://latex.codecogs.com/gif.latex?r_{d}^{t}&space;=&space;\begin{cases}&space;-0.25&space;&amp;&space;\text{&space;if&space;}&space;\exists{o&space;\in&space;O}&space;:&space;d(p_{robot}^t,&space;p_{obs}^t)&space;&lt;&space;D_{s}\&space;0&space;&amp;&space;\text{&space;otherwise&space;}&space;\end{cases}" title="r_{d}^{t} = \begin{cases} -0.25 &amp; \text{ if } \exists{o \in O} : d(p_{robot}^t, p_{obs}^t) &lt; D_{s}\ 0 &amp; \text{ otherwise } \end{cases}" /&gt;</td>
</tr>
<tr>
<td><img src="https://latex.codecogs.com/gif.latex?r_{p}^{t}" title="r_{p}^{t}" /></td>
<td>progress reward</td>
<td><img src="https://latex.codecogs.com/gif.latex?\text{diff}_{robot,x}^t&space;=&space;d(p_{robot}^{t-1},&space;p_{x}^{t-1})&space;-&space;d(p_{robot}^t,&space;p_{x}^t)" title="\text{diff}_{robot,x}^t = d(p_{robot}^{t-1}, p_{x}^{t-1}) - d(p_{robot}^t, p_{x}^t)" /> <img src="https://latex.codecogs.com/gif.latex?r_{p}^{t}&space;=&space;\begin{cases}&space;0.3&space;*&space;\text{diff}_{robot,goal}^t&space;&&space;\text{&space;if&space;}&space;\text{diff}_{robot,goal}^t&space;>&space;0\&space;0.4&space;*&space;\text{diff}<em>{robot,goal}^t&space;&amp;&space;\text{&space;otherwise&space;}&space;\end{cases}" title="r</em>{p}^{t} = \begin{cases} 0.3 * \text{diff}<em>{robot,goal}^t &amp; \text{ if } \text{diff}</em>{robot,goal}^t &gt; 0\ 0.4 * \text{diff}_{robot,goal}^t &amp; \text{ otherwise } \end{cases}" /&gt;</td>
</tr>
<tr>
<td><img src="https://latex.codecogs.com/gif.latex?r_{dt}^{t}" title="r_{dt}^{t}" /></td>
<td>distance travelled reward</td>
<td><img src="https://latex.codecogs.com/gif.latex?r_{dt}^{t}&space;=&space;(vel_{linear}^{t}&space;&plus;&space;(vel_{angular}^{t}*0.001))*-0.0075" title="r_{dt}^{t} = (vel_{linear}^{t} + (vel_{angular}^{t}*0.001))*-0.0075" /></td>
</tr>
</tbody>
</table>
<h4 id="rule-2">Rule 2</h4>
<p><img src="https://latex.codecogs.com/gif.latex?r_{02}^{t}&space;=&space;r_{s}^{t}&space;&plus;&space;r_{c}^{t}&space;&plus;&space;r_{d}^{t}&space;&plus;&space;r_{p}^{t}&space;&plus;&space;r_{dt}^{t}&space;&plus;&space;r_{fg}^{t}" title="r_{02}^{t} = r_{s}^{t} + r_{c}^{t} + r_{d}^{t} + r_{p}^{t} + r_{dt}^{t} + r_{fg}^{t}" /></p>
<table>
<thead>
<tr>
<th>reward</th>
<th>description</th>
<th>value</th>
</tr>
</thead>
<tbody>
<tr>
<td><img src="https://latex.codecogs.com/gif.latex?r_{s}^{t}" title="r_{s}^{t}" /></td>
<td>success reward</td>
<td><img src="https://latex.codecogs.com/gif.latex?r_{s}^{t}&space;=&space;\begin{cases}&space;15&space;&&space;\text{&space;if&space;goal&space;reached}&space;\92&space;0&space;&&space;\text{&space;otherwise&space;}&space;\end{cases}" title="r_{s}^{t} = \begin{cases} 15 & \text{ if goal reached} \92 0 & \text{ otherwise } \end{cases}" /></td>
</tr>
<tr>
<td><img src="https://latex.codecogs.com/gif.latex?r_{c}^{t}" title="r_{c}^{t}" /></td>
<td>collision reward</td>
<td><img src="https://latex.codecogs.com/gif.latex?r_{c}^{t}&space;=&space;\begin{cases}&space;-10&space;&&space;\text{&space;if&space;robot&space;collides}&space;\92&space;0&space;&&space;\text{&space;otherwise&space;}&space;\end{cases}" title="r_{c}^{t} = \begin{cases} -10 & \text{ if robot collides} \92 0 & \text{ otherwise } \end{cases}" /></td>
</tr>
<tr>
<td><img src="https://latex.codecogs.com/gif.latex?r_{d}^{t}" title="r_{d}^{t}" /></td>
<td>danger reward</td>
<td>&lt;img src="https://latex.codecogs.com/gif.latex?r_{d}^{t}&space;=&space;\begin{cases}&space;-0.25&space;&amp;&space;\text{&space;if&space;}&space;\exists{o&space;\in&space;O}&space;:&space;d(p_{robot}^t,&space;p_{obs}^t)&space;&lt;&space;D_{s}\&space;0&space;&amp;&space;\text{&space;otherwise&space;}&space;\end{cases}" title="r_{d}^{t} = \begin{cases} -0.25 &amp; \text{ if } \exists{o \in O} : d(p_{robot}^t, p_{obs}^t) &lt; D_{s}\ 0 &amp; \text{ otherwise } \end{cases}" /&gt;</td>
</tr>
<tr>
<td><img src="https://latex.codecogs.com/gif.latex?r_{p}^{t}" title="r_{p}^{t}" /></td>
<td>progress reward</td>
<td><img src="https://latex.codecogs.com/gif.latex?\text{diff}_{robot,x}^t&space;=&space;d(p_{robot}^{t-1},&space;p_{x}^{t-1})&space;-&space;d(p_{robot}^t,&space;p_{x}^t)" title="\text{diff}_{robot,x}^t = d(p_{robot}^{t-1}, p_{x}^{t-1}) - d(p_{robot}^t, p_{x}^t)" /> <img src="https://latex.codecogs.com/gif.latex?r_{p}^{t}&space;=&space;\begin{cases}&space;0.3&space;*&space;\text{diff}_{robot,goal}^t&space;&&space;\text{&space;if&space;}&space;\text{diff}_{robot,goal}^t&space;>&space;0\&space;0.4&space;*&space;\text{diff}<em>{robot,goal}^t&space;&amp;&space;\text{&space;otherwise&space;}&space;\end{cases}" title="r</em>{p}^{t} = \begin{cases} 0.3 * \text{diff}<em>{robot,goal}^t &amp; \text{ if } \text{diff}</em>{robot,goal}^t &gt; 0\ 0.4 * \text{diff}_{robot,goal}^t &amp; \text{ otherwise } \end{cases}" /&gt;</td>
</tr>
<tr>
<td><img src="https://latex.codecogs.com/gif.latex?r_{dt}^{t}" title="r_{dt}^{t}" /></td>
<td>distance travelled reward</td>
<td><img src="https://latex.codecogs.com/gif.latex?r_{dt}^{t}&space;=&space;(vel_{linear}^{t}&space;&plus;&space;(vel_{angular}^{t}*0.001))*-0.0075" title="r_{dt}^{t} = (vel_{linear}^{t} + (vel_{angular}^{t}*0.001))*-0.0075" /></td>
</tr>
<tr>
<td><img src="https://latex.codecogs.com/gif.latex?r_{fg}^{t}" title="r_{fg}^{t}" /></td>
<td>following global plan reward</td>
<td>&lt;img src="https://latex.codecogs.com/gif.latex?r_{fg}^{t}&space;=&space;\begin{cases}&space;\begin{aligned}&space;0.1&space;*&space;vel_{linear}^{t}&space;&amp;&space;\text{&space;if&space;}&space;\min_{wp&space;\in&space;G}d(p_{wp}^t,&space;p_{r}^t)&space;&lt;&space;0.5&space;\text{m}&space;\&space;0&space;&amp;&space;\text{&space;otherwise&space;}&space;\end{aligned}&space;\end{cases}" title="r_{fg}^{t} = \begin{cases} \begin{aligned} 0.1 * vel_{linear}^{t} &amp; \text{ if } \min_{wp \in G}d(p_{wp}^t, p_{r}^t) &lt; 0.5 \text{m} \ 0 &amp; \text{ otherwise } \end{aligned} \end{cases}" /&gt;</td>
</tr>
</tbody>
</table>
<h4 id="rule-3">Rule 3</h4>
<p><img src="https://latex.codecogs.com/gif.latex?r_{03}^{t}&space;=&space;r_{s}^{t}&space;&plus;&space;r_{c}^{t}&space;&plus;&space;r_{d}^{t}&space;&plus;&space;r_{p}^{t}&space;&plus;&space;r_{fg}^{t}&space;&plus;&space;r_{dg}^{t}" title="r_{03}^{t} = r_{s}^{t} + r_{c}^{t} + r_{d}^{t} + r_{p}^{t} + r_{fg}^{t} + r_{dg}^{t}" /></p>
<table>
<thead>
<tr>
<th>reward</th>
<th>description</th>
<th>value</th>
</tr>
</thead>
<tbody>
<tr>
<td><img src="https://latex.codecogs.com/gif.latex?r_{s}^{t}" title="r_{s}^{t}" /></td>
<td>success reward</td>
<td><img src="https://latex.codecogs.com/gif.latex?r_{s}^{t}&space;=&space;\begin{cases}&space;15&space;&&space;\text{&space;if&space;goal&space;reached}&space;\92&space;0&space;&&space;\text{&space;otherwise&space;}&space;\end{cases}" title="r_{s}^{t} = \begin{cases} 15 & \text{ if goal reached} \92 0 & \text{ otherwise } \end{cases}" /></td>
</tr>
<tr>
<td><img src="https://latex.codecogs.com/gif.latex?r_{c}^{t}" title="r_{c}^{t}" /></td>
<td>collision reward</td>
<td><img src="https://latex.codecogs.com/gif.latex?r_{c}^{t}&space;=&space;\begin{cases}&space;-10&space;&&space;\text{&space;if&space;robot&space;collides}&space;\92&space;0&space;&&space;\text{&space;otherwise&space;}&space;\end{cases}" title="r_{c}^{t} = \begin{cases} -10 & \text{ if robot collides} \92 0 & \text{ otherwise } \end{cases}" /></td>
</tr>
<tr>
<td><img src="https://latex.codecogs.com/gif.latex?r_{d}^{t}" title="r_{d}^{t}" /></td>
<td>danger reward</td>
<td>&lt;img src="https://latex.codecogs.com/gif.latex?r_{d}^{t}&space;=&space;\begin{cases}&space;-0.25&space;&amp;&space;\text{&space;if&space;}&space;\exists{o&space;\in&space;O}&space;:&space;d(p_{robot}^t,&space;p_{obs}^t)&space;&lt;&space;D_{s}\&space;0&space;&amp;&space;\text{&space;otherwise&space;}&space;\end{cases}" title="r_{d}^{t} = \begin{cases} -0.25 &amp; \text{ if } \exists{o \in O} : d(p_{robot}^t, p_{obs}^t) &lt; D_{s}\ 0 &amp; \text{ otherwise } \end{cases}" /&gt;</td>
</tr>
<tr>
<td><img src="https://latex.codecogs.com/gif.latex?r_{p}^{t}" title="r_{p}^{t}" /></td>
<td>progress reward</td>
<td><img src="https://latex.codecogs.com/gif.latex?\text{diff}_{robot,x}^t&space;=&space;d(p_{robot}^{t-1},&space;p_{x}^{t-1})&space;-&space;d(p_{robot}^t,&space;p_{x}^t)" title="\text{diff}_{robot,x}^t = d(p_{robot}^{t-1}, p_{x}^{t-1}) - d(p_{robot}^t, p_{x}^t)" /> <img src="https://latex.codecogs.com/gif.latex?r_{p}^{t}&space;=&space;\begin{cases}&space;0.3&space;*&space;\text{diff}_{robot,goal}^t&space;&&space;\text{&space;if&space;}&space;\text{diff}_{robot,goal}^t&space;>&space;0\&space;0.4&space;*&space;\text{diff}<em>{robot,goal}^t&space;&amp;&space;\text{&space;otherwise&space;}&space;\end{cases}" title="r</em>{p}^{t} = \begin{cases} 0.3 * \text{diff}<em>{robot,goal}^t &amp; \text{ if } \text{diff}</em>{robot,goal}^t &gt; 0\ 0.4 * \text{diff}_{robot,goal}^t &amp; \text{ otherwise } \end{cases}" /&gt;</td>
</tr>
<tr>
<td><img src="https://latex.codecogs.com/gif.latex?r_{fg}^{t}" title="r_{fg}^{t}" /></td>
<td>following global plan reward</td>
<td>&lt;img src="https://latex.codecogs.com/gif.latex?r_{fg}^{t}&space;=&space;\begin{cases}&space;\begin{aligned}&space;0.1&space;*&space;vel_{linear}^{t}&space;&amp;&space;\text{&space;if&space;}&space;\min_{wp&space;\in&space;G}d(p_{wp}^t,&space;p_{r}^t)&space;&lt;&space;0.5&space;\text{m}&space;\&space;0&space;&amp;&space;\text{&space;otherwise&space;}&space;\end{aligned}&space;\end{cases}" title="r_{fg}^{t} = \begin{cases} \begin{aligned} 0.1 * vel_{linear}^{t} &amp; \text{ if } \min_{wp \in G}d(p_{wp}^t, p_{r}^t) &lt; 0.5 \text{m} \ 0 &amp; \text{ otherwise } \end{aligned} \end{cases}" /&gt;</td>
</tr>
<tr>
<td><img src="https://latex.codecogs.com/gif.latex?r_{dg}" title="r_{dg}" /></td>
<td>distance to globalplan reward</td>
<td><img src="https://latex.codecogs.com/gif.latex?r_{dg}&space;=&space;\begin{cases}&space;\begin{aligned}&space;0.2*&space;\text{diff}_{robot,&space;wp}^{t}&space;&&space;\text{&space;if&space;}\min_{wp&space;\in&space;G}d(p_{r}^t,&space;p_{wp}^t):&space;\text{diff}_{robot,&space;wp}^{t}&space;>&space;0&space;\&space;0.3<em>&space;\text{diff}<em>{robot,&space;wp}^{t}&space;&amp;&space;\text{&space;if&space;}&space;\min</em>{wp&space;\in&space;G}d(p_{r}^t,&space;p_{wp}^t):&space;\text{diff}<em>{robot,&space;wp}^{t}&space;&lt;=&space;0&space;\&space;0&space;&amp;&space;\text{&space;if&space;}&space;\min</em>{o&space;\in&space;O}d(p_{r}^t,&space;p_{o}^t)&space;&lt;&space;D_s&space;\end{aligned}&space;\end{cases}" title="r_{dg} = \begin{cases} \begin{aligned} 0.2</em> \text{diff}<em>{robot, wp}^{t} &amp; \text{ if }\min</em>{wp \in G}d(p_{r}^t, p_{wp}^t): \text{diff}<em>{robot, wp}^{t} &gt; 0 \ 0.3* \text{diff}</em>{robot, wp}^{t} &amp; \text{ if } \min_{wp \in G}d(p_{r}^t, p_{wp}^t): \text{diff}<em>{robot, wp}^{t} &lt;= 0 \ 0 &amp; \text{ if } \min</em>{o \in O}d(p_{r}^t, p_{o}^t) &lt; D_s \end{aligned} \end{cases}" /&gt;</td>
</tr>
</tbody>
</table>
<h4 id="rule-4">Rule 4</h4>
<p><img src="https://latex.codecogs.com/gif.latex?r_{04}^{t}&space;=&space;r_{s}^{t}&space;&plus;&space;r_{c}^{t}&space;&plus;&space;r_{d}^{t}&space;&plus;&space;r_{p}^{t}&space;&plus;&space;r_{fg}^{t}&space;&plus;&space;r_{dg}^{t}&space;&plus;&space;r_{dc}^{t}" title="r_{04}^{t} = r_{s}^{t} + r_{c}^{t} + r_{d}^{t} + r_{p}^{t} + r_{fg}^{t} + r_{dg}^{t} + r_{dc}^{t}" /></p>
<table>
<thead>
<tr>
<th>reward</th>
<th>description</th>
<th>value</th>
</tr>
</thead>
<tbody>
<tr>
<td><img src="https://latex.codecogs.com/gif.latex?r_{s}^{t}" title="r_{s}^{t}" /></td>
<td>success reward</td>
<td><img src="https://latex.codecogs.com/gif.latex?r_{s}^{t}&space;=&space;\begin{cases}&space;15&space;&&space;\text{&space;if&space;goal&space;reached}&space;\92&space;0&space;&&space;\text{&space;otherwise&space;}&space;\end{cases}" title="r_{s}^{t} = \begin{cases} 15 & \text{ if goal reached} \92 0 & \text{ otherwise } \end{cases}" /></td>
</tr>
<tr>
<td><img src="https://latex.codecogs.com/gif.latex?r_{c}^{t}" title="r_{c}^{t}" /></td>
<td>collision reward</td>
<td><img src="https://latex.codecogs.com/gif.latex?r_{c}^{t}&space;=&space;\begin{cases}&space;-10&space;&&space;\text{&space;if&space;robot&space;collides}&space;\92&space;0&space;&&space;\text{&space;otherwise&space;}&space;\end{cases}" title="r_{c}^{t} = \begin{cases} -10 & \text{ if robot collides} \92 0 & \text{ otherwise } \end{cases}" /></td>
</tr>
<tr>
<td><img src="https://latex.codecogs.com/gif.latex?r_{d}^{t}" title="r_{d}^{t}" /></td>
<td>danger reward</td>
<td>&lt;img src="https://latex.codecogs.com/gif.latex?r_{d}^{t}&space;=&space;\begin{cases}&space;-0.25&space;&amp;&space;\text{&space;if&space;}&space;\exists{o&space;\in&space;O}&space;:&space;d(p_{robot}^t,&space;p_{obs}^t)&space;&lt;&space;D_{s}\&space;0&space;&amp;&space;\text{&space;otherwise&space;}&space;\end{cases}" title="r_{d}^{t} = \begin{cases} -0.25 &amp; \text{ if } \exists{o \in O} : d(p_{robot}^t, p_{obs}^t) &lt; D_{s}\ 0 &amp; \text{ otherwise } \end{cases}" /&gt;</td>
</tr>
<tr>
<td><img src="https://latex.codecogs.com/gif.latex?r_{p}^{t}" title="r_{p}^{t}" /></td>
<td>progress reward</td>
<td><img src="https://latex.codecogs.com/gif.latex?\text{diff}_{robot,x}^t&space;=&space;d(p_{robot}^{t-1},&space;p_{x}^{t-1})&space;-&space;d(p_{robot}^t,&space;p_{x}^t)" title="\text{diff}_{robot,x}^t = d(p_{robot}^{t-1}, p_{x}^{t-1}) - d(p_{robot}^t, p_{x}^t)" /> <img src="https://latex.codecogs.com/gif.latex?r_{p}^{t}&space;=&space;\begin{cases}&space;0.3&space;*&space;\text{diff}_{robot,goal}^t&space;&&space;\text{&space;if&space;}&space;\text{diff}_{robot,goal}^t&space;>&space;0\&space;0.4&space;*&space;\text{diff}<em>{robot,goal}^t&space;&amp;&space;\text{&space;otherwise&space;}&space;\end{cases}" title="r</em>{p}^{t} = \begin{cases} 0.3 * \text{diff}<em>{robot,goal}^t &amp; \text{ if } \text{diff}</em>{robot,goal}^t &gt; 0\ 0.4 * \text{diff}_{robot,goal}^t &amp; \text{ otherwise } \end{cases}" /&gt;</td>
</tr>
<tr>
<td><img src="https://latex.codecogs.com/gif.latex?r_{fg}^{t}" title="r_{fg}^{t}" /></td>
<td>following global plan reward</td>
<td>&lt;img src="https://latex.codecogs.com/gif.latex?r_{fg}^{t}&space;=&space;\begin{cases}&space;\begin{aligned}&space;0.1&space;*&space;vel_{linear}^{t}&space;&amp;&space;\text{&space;if&space;}&space;\min_{wp&space;\in&space;G}d(p_{wp}^t,&space;p_{r}^t)&space;&lt;&space;0.5&space;\text{m}&space;\&space;0&space;&amp;&space;\text{&space;otherwise&space;}&space;\end{aligned}&space;\end{cases}" title="r_{fg}^{t} = \begin{cases} \begin{aligned} 0.1 * vel_{linear}^{t} &amp; \text{ if } \min_{wp \in G}d(p_{wp}^t, p_{r}^t) &lt; 0.5 \text{m} \ 0 &amp; \text{ otherwise } \end{aligned} \end{cases}" /&gt;</td>
</tr>
<tr>
<td><img src="https://latex.codecogs.com/gif.latex?r_{dg}" title="r_{dg}" /></td>
<td>distance to globalplan reward</td>
<td><img src="https://latex.codecogs.com/gif.latex?r_{dg}&space;=&space;\begin{cases}&space;\begin{aligned}&space;0.2*&space;\text{diff}_{robot,&space;wp}^{t}&space;&&space;\text{&space;if&space;}\min_{wp&space;\in&space;G}d(p_{r}^t,&space;p_{wp}^t):&space;\text{diff}_{robot,&space;wp}^{t}&space;>&space;0&space;\&space;0.3<em>&space;\text{diff}<em>{robot,&space;wp}^{t}&space;&amp;&space;\text{&space;if&space;}&space;\min</em>{wp&space;\in&space;G}d(p_{r}^t,&space;p_{wp}^t):&space;\text{diff}<em>{robot,&space;wp}^{t}&space;&lt;=&space;0&space;\&space;0&space;&amp;&space;\text{&space;if&space;}&space;\min</em>{o&space;\in&space;O}d(p_{r}^t,&space;p_{o}^t)&space;&lt;&space;D_s&space;\end{aligned}&space;\end{cases}" title="r_{dg} = \begin{cases} \begin{aligned} 0.2</em> \text{diff}<em>{robot, wp}^{t} &amp; \text{ if }\min</em>{wp \in G}d(p_{r}^t, p_{wp}^t): \text{diff}<em>{robot, wp}^{t} &gt; 0 \ 0.3* \text{diff}</em>{robot, wp}^{t} &amp; \text{ if } \min_{wp \in G}d(p_{r}^t, p_{wp}^t): \text{diff}<em>{robot, wp}^{t} &lt;= 0 \ 0 &amp; \text{ if } \min</em>{o \in O}d(p_{r}^t, p_{o}^t) &lt; D_s \end{aligned} \end{cases}" /&gt;</td>
</tr>
</tbody>
</table>
<p>| <img src="https://latex.codecogs.com/gif.latex?r_{dc}^t" title="r_{dc}^t" /> | direction change reward | <img src="https://latex.codecogs.com/gif.latex?r_{dc}^t&space;=&space;-&space;\frac{\left&space;|&space;vel*{angular}^{t-1}&space;-&space;vel*{angular}^{t}&space;\right&space;\124^{4}}{2500}" title="r\95{dc}^t = - \frac{\left \124 vel*{angular}^{t-1} - vel*{angular}^{t} \right | ^{4}}{2500}" /> |</p>
</td>
<p></tr>
</table></p>
<h2 id="training-curriculum">Training Curriculum</h2>
<p>For the purpose of speeding up the training, an exemplary training currucilum was implemented. But what exactly is a training curriculum you may ask. We basically divide the training process in difficulty levels, here the so called <em>stages</em>, in which the agent will meet an arbitrary number of obstacles depending on its learning progress. Different metrics can be taken into consideration to measure an agents performance.</p>
<p>In our implementation a reward threshold or a certain percentage of successful episodes must be reached to trigger the next stage. The statistics of each evaluation run is calculated and considered. Moreover, when a new best mean reward was reached, the model will be saved automatically.</p>
<p>Currently, the threshold type and respective values can be set in the following line: <a href="https://github.com/ignc-research/arena-rosnav/blob/a7c17cbbf172afc1421a7943fce8f0ba986dbead/arena_navigation/arena_local_planner/learning_based/arena_local_planner_drl/scripts/training/train_agent.py#L70">click here</a></p>
<p>Exemplary training curriculum:</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Stage</th>
<th style="text-align: center;">Static Obstacles</th>
<th style="text-align: center;">Dynamic Obstacles</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">10</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">10</td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">13</td>
</tr>
</tbody>
</table>
<p>For an explicit example, <a href="/arena-rosnav/arena_navigation/arena_local_planner/learning_based/arena_local_planner_drl/configs/training_curriculum_map1small.yaml">click here</a>.</p>
<h2 id="run-the-trained-agent">Run the trained Agent</h2>
<p>Now that you've trained your agent, you surely want to deploy and evaluate it. For that purpose we've implemented a specific task mode in which you can specify your scenarios in a .json file. The agent will then be challenged according to the scenarios defined in the file. Please refer to https://github.com/ignc-research/arena-scenario-gui/ in order to read about the process of creating custom scenarios.
Moreover, you can test your agent on custom maps in randomly generated scenarios with a predefined number of dynamic obstacles.</p>
<p>As with the training script, one can start the testing simulation environment with either one of two launch scripts:</p>
<ul>
<li>start_arena_flatland.launch:</li>
<li>Allows for evaluation in continuous simulation time (emulates real time) as well as in controlled time stepping with our hierarichal navigation stack, consisting of a global planner, intermediate planner and with the DRL agent acting as the local planner.</li>
<li>Episode information can be logged with the <em>use_recorder</em> flag</li>
<li>start_training.launch:</li>
<li>Starts an evaluation environment in continuous simulation time (emulates real time) as well as in controlled time stepping with either the spatial horizon intermediate planner (train_mode == false) or the end goal being the only subgoal (train_mode == true).</li>
<li>One can test multiple agents sequentially with <em>run_script.py</em>. This feature is only realized with this launch file, as <em>start_arena_flatland.launch</em> starts an own plan manager which interfers with the plan manager of the run script. Both plan managers have their own goal radius and thus might detect an end of episode differently. This potentially adulterates the logged statistics.</li>
<li>Episode information can optionally be logged in a csv file by setting the <code>--log</code> flag for the run script dedicated plan manager to control the episodes.</li>
</ul>
<h3 id="test-agents-in-main-simulation">Test Agents in Main Simulation</h3>
<p>The deployment can be simply initiated through one command:</p>
<pre><code class="language-shell"># Start the simulation with one of the launch files
roslaunch arena_bringup start_arena_flatland.launch map_file:=&quot;map1&quot;  disable_scenario:=&quot;false&quot; scenario_file:=&quot;eval/obstacle_map1_obs20.json&quot; local_planner:=&quot;rosnav&quot; model:=&quot;burger&quot; agent_name:=&quot;burger&quot;
</code></pre>
<p><strong>Note</strong>:
You need to adjust the parameters of the command according to your desired deployment configuration.</p>
<h3 id="test-agents-in-training-simulation">Test Agents in Training Simulation</h3>
<p>Firstly, you need to start the <em>simulation environment</em>:</p>
<pre><code class="language-shell"># Start the simulation with one of the launch files
roslaunch arena_bringup start_training.launch num_envs:=1 map_folder_name:=map1 train_mode:=false model:=burger
</code></pre>
<p><strong>Note</strong>:</p>
<ul>
<li>The <code>train_mode</code> parameter determines if the simulation will run in emulated real time (where <code>step_size</code> and <code>update_rate</code> determine the simulation speed) or in the manipulating time stepping modus (via <em>/step_world</em> rostopic).</li>
<li>Make sure that you start the simulation with the matching robot model for the agent</li>
</ul>
<p></br></p>
<p>Then, run the <code>run_agent.py</code> script with the desired agent and scenario file:</p>
<pre><code class="language-shell">python run_agent.py --load burger --scenario obstacle_map1_obs20
</code></pre>
<p><strong>Generic program call</strong>:</p>
<pre><code>roscd arena_local_planner_drl/scripts/deployment/
run_agent.py --load [agent_name] -s [scenario_name] -v [number] [optional flag]
</code></pre>
<table>
<thead>
<tr>
<th>Program call</th>
<th>Flags</th>
<th>Usage</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>run_agent.py</code></td>
<td><code>--load</code></td>
<td><em>agent_name</em> (<a href="#load-a-dnn-for-training">see below</a>)</td>
<td>loads agent to the given name</td>
</tr>
<tr>
<td></td>
<td><code>-s</code> or <code>--scenario</code></td>
<td><em>scenario_name</em> (as in <em>../scenario/eval/</em>)</td>
<td>loads the scenarios to the given .json file name</td>
</tr>
<tr>
<td></td>
<td>(optional)<code>-v</code> or <code>--verbose</code></td>
<td><em>0 or 1</em></td>
<td>verbose level</td>
</tr>
<tr>
<td></td>
<td>(optional) <code>--no-gpu</code></td>
<td><em>None</em></td>
<td>disables the gpu for the evaluation</td>
</tr>
<tr>
<td></td>
<td>(optional) <code>--num_eps</code></td>
<td><em>Integer</em>, defaults to 100</td>
<td>number of episodes the agent/s get/s challenged</td>
</tr>
<tr>
<td></td>
<td>(optional) <code>--max_steps</code></td>
<td><em>Integer</em>, defaults to np.inf</td>
<td>max amount of actions per episode, before the episode is resetted automatically</td>
</tr>
</tbody>
</table>
<ul>
<li>Example call:</li>
</ul>
<pre><code>python run_agent.py --load DRL_LOCAL_PLANNER_2021_03_22__19_33 -s obstacle_map1_obs20
</code></pre>
<p><strong>Notes</strong>:</p>
<ul>
<li>The <code>--log</code> flag should only be set with <em>start_training.launch</em> as simulation launcher as it requires the dedicated plan manager to control the episodes in order to log correct statistics.</li>
<li>Make sure that the simulation speed doesn't overlap the agent's action calculation time (an obvious indicator: same action gets published multiple times successively and thus the agent moves unreasonably)</li>
<li>If your agent was trained with normalized observations, it's necessary to provide the <em>vec_normalize.pkl</em></li>
</ul>
<h4 id="sequential-evaluation-of-multiple-agents">Sequential Evaluation of multiple Agents</h4>
<p>For automatic testing of several agents in a sequence, one can specify a list containing an arbitrary number of agent names in <a href="/arena-rosnav/arena_navigation/arena_local_planner/learning_based/arena_local_planner_drl/scripts/deployment/run_agent.py">run_script.py</a>.</p>
<p><strong>Note</strong>:</p>
<ul>
<li>Guaranteed execution of each agent is currently only provided with the <em>start_training.launch</em> as simulation launcher</li>
<li><code>--load</code> flag has to be set <em>None</em>, otherwise the script will only consider the agent provided with the flag.</li>
</ul>
<h2 id="pipeline-components">Pipeline Components</h2>
<p><img alt="drl_training_modules" src="../../images/drl_training_modules.png" /></p>
<h2 id="important-directories">Important Directories</h2>
<table>
<thead>
<tr>
<th>Path</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>../arena_local_planner_drl/agents</code></td>
<td>models and associated hyperparameters.json will be saved to and loaded from here (<a href="#load-a-dnn-for-training">uniquely named directory</a>)</td>
</tr>
<tr>
<td><code>../arena_local_planner_drl/configs</code></td>
<td>yaml files containing robots action spaces and the training curriculum</td>
</tr>
<tr>
<td><code>../arena_local_planner_drl/training_logs</code></td>
<td>tensorboard logs and evaluation logs</td>
</tr>
<tr>
<td><code>../arena_local_planner_drl/scripts</code></td>
<td>python file containing the predefined DNN architectures and the training script</td>
</tr>
</tbody>
</table>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../usage/" class="btn btn-neutral float-left" title="Usage"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../../packages/task_generator/" class="btn btn-neutral float-right" title="Task Generator">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../usage/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../../packages/task_generator/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script>var base_url = '../..';</script>
    <script src="../../js/theme_extra.js" defer></script>
    <script src="../../js/theme.js" defer></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.6.0/clipboard.min.js" defer></script>
      <script src="../../js/cpybtn.js" defer></script>
    <script defer>
        window.onload = function () {
            SphinxRtdTheme.Navigation.enable(true);
        };
    </script>

</body>
</html>
